"""
NTIS ê³µê³  ë°ì´í„° ê´€ë¦¬ì
- 30ê°œ ê³ ì • í¬ê¸° ìœ ì§€
- ìƒˆë¡œìš´ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„° ë¹„êµ
- ìƒì„¸ í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬
"""

import json
import os
import re
from datetime import datetime
from typing import List, Dict, Optional, Tuple

class NTISDataManager:
    """NTIS ê³µê³  ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, json_file_path: str = "output/ntis_managed_data.json"):
        self.json_file_path = json_file_path
        self.max_items = 30  # ìµœëŒ€ 30ê°œ ìœ ì§€
        self.data_structure = {
            "last_updated": "",
            "search_keyword": "",
            "total_count": 0,
            "announcements": []
        }
    
    def extract_uid_from_url(self, url: str) -> str:
        """ìƒì„¸_URLì—ì„œ roRndUid ì¶”ì¶œ"""
        try:
            # roRndUid=ìˆ«ì íŒ¨í„´ ì°¾ê¸°
            match = re.search(r'roRndUid=(\d+)', url)
            if match:
                return match.group(1)
            else:
                return ""
        except Exception:
            return ""
    
    def load_existing_data(self) -> Dict:
        """ê¸°ì¡´ JSON ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.json_file_path):
                with open(self.json_file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {len(data.get('announcements', []))}ê°œ")
                    return data
            else:
                print("ğŸ“„ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ, ìƒˆë¡œ ì‹œì‘")
                return self.data_structure.copy()
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return self.data_structure.copy()
    
    def save_data(self, data: Dict) -> bool:
        """ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # output í´ë” ìƒì„±
            os.makedirs(os.path.dirname(self.json_file_path), exist_ok=True)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
            data["last_updated"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            with open(self.json_file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.json_file_path}")
            return True
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def compare_and_find_new(self, new_crawled_data: List[Dict], keyword: str) -> Tuple[List[Dict], List[Dict]]:
        """ìƒˆ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ì‹ ê·œ í•­ëª© ì°¾ê¸°"""
        print(f"\nğŸ” ë°ì´í„° ë¹„êµ ì‹œì‘ (í‚¤ì›Œë“œ: {keyword})")
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_data = self.load_existing_data()
        existing_announcements = existing_data.get("announcements", [])
        
        # ê¸°ì¡´ ê³µê³  UIDë“¤ ì¶”ì¶œ (roRndUid ê¸°ì¤€)
        existing_uids = set()
        for item in existing_announcements:
            uid = self.extract_uid_from_url(item.get("ìƒì„¸_URL", ""))
            if uid:
                existing_uids.add(uid)
        
        print(f"   ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {len(existing_uids)}ê°œ")
        print(f"   ğŸ“Š ìƒˆ ë°ì´í„°: {len(new_crawled_data)}ê°œ")
        
        # ì‹ ê·œ í•­ëª©ê³¼ ê¸°ì¡´ í•­ëª© ë¶„ë¥˜
        new_items = []
        existing_items = []
        
        for item in new_crawled_data:
            item_uid = self.extract_uid_from_url(item.get("ìƒì„¸_URL", ""))
            
            # ë°ì´í„° êµ¬ì¡° í‘œì¤€í™”
            standardized_item = {
                "í˜„í™©": item.get("í˜„í™©", ""),
                "ê³µê³ ëª…": item.get("ê³µê³ ëª…", ""),
                "ë¶€ì²˜ëª…": item.get("ë¶€ì²˜ëª…", ""),
                "ì ‘ìˆ˜ì¼": item.get("ì ‘ìˆ˜ì¼", ""),
                "ë§ˆê°ì¼": item.get("ë§ˆê°ì¼", ""),
                "ìƒì„¸_URL": item.get("ìƒì„¸_URL", ""),
                "onclick": item.get("onclick", ""),
                "D_day": item.get("D_day", ""),
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "is_detailed": False,
                "detail_data": {}
            }
            
            if item_uid and item_uid not in existing_uids:
                new_items.append(standardized_item)
                print(f"   ğŸ†• ì‹ ê·œ ë°œê²¬: [UID:{item_uid}] {item.get('ê³µê³ ëª…', '')[:50]}...")
            else:
                existing_items.append(standardized_item)
        
        print(f"   âœ… ë¹„êµ ì™„ë£Œ: ì‹ ê·œ {len(new_items)}ê°œ, ê¸°ì¡´ {len(existing_items)}ê°œ")
        
        return new_items, existing_items
    
    def update_data_with_new_items(self, new_items: List[Dict], all_current_items: List[Dict], keyword: str) -> Dict:
        """ìƒˆ í•­ëª©ì„ ì¶”ê°€í•˜ê³  30ê°œë¡œ ì œí•œí•˜ì—¬ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        print(f"\nğŸ“ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘")
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_data = self.load_existing_data()
        
        # ì „ì²´ í˜„ì¬ í•­ëª©ë“¤ì„ ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬ (ì ‘ìˆ˜ì¼ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
        try:
            # ì ‘ìˆ˜ì¼ í˜•íƒœ: "2025.09.08" -> datetime ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
            def parse_date(date_str):
                try:
                    return datetime.strptime(date_str, "%Y.%m.%d")
                except:
                    return datetime.min  # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œë¡œ ì²˜ë¦¬
            
            all_current_items.sort(key=lambda x: parse_date(x.get("ì ‘ìˆ˜ì¼", "")), reverse=True)
            print(f"   ğŸ“… ì ‘ìˆ˜ì¼ ê¸°ì¤€ ì •ë ¬ ì™„ë£Œ (ìµœì‹ ìˆœ)")
        except Exception as e:
            print(f"   âš ï¸ ì ‘ìˆ˜ì¼ ì •ë ¬ ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€: {str(e)}")
            # ì ‘ìˆ˜ì¼ ì •ë ¬ì´ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ìˆœì„œ ìœ ì§€
        
        # ìµœì‹  30ê°œë§Œ ìœ ì§€
        final_items = all_current_items[:self.max_items]
        
        # ì œê±°ëœ í•­ëª© ê°œìˆ˜ ê³„ì‚°
        removed_count = len(all_current_items) - len(final_items)
        
        # ë°ì´í„° êµ¬ì¡° ì—…ë°ì´íŠ¸
        updated_data = {
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "search_keyword": keyword,
            "total_count": len(final_items),
            "new_items_count": len(new_items),
            "removed_old_count": removed_count,
            "announcements": final_items
        }
        
        print(f"   ğŸ“Š ì—…ë°ì´íŠ¸ ê²°ê³¼:")
        print(f"      ğŸ†• ì‹ ê·œ ì¶”ê°€: {len(new_items)}ê°œ")
        print(f"      ğŸ—‘ï¸ ì˜¤ë˜ëœ í•­ëª© ì œê±°: {removed_count}ê°œ")
        print(f"      ğŸ“‹ ìµœì¢… ìœ ì§€: {len(final_items)}ê°œ")
        
        return updated_data
    
    def get_items_for_detail_crawling(self, data: Dict) -> List[Dict]:
        """ìƒì„¸ í¬ë¡¤ë§ì´ í•„ìš”í•œ í•­ëª©ë“¤ ë°˜í™˜"""
        items_to_crawl = []
        
        for item in data.get("announcements", []):
            if not item.get("is_detailed", False):
                items_to_crawl.append(item)
        
        print(f"ğŸ” ìƒì„¸ í¬ë¡¤ë§ í•„ìš” í•­ëª©: {len(items_to_crawl)}ê°œ")
        return items_to_crawl
    
    def mark_as_detailed(self, data: Dict, ìˆœë²ˆ: str, detail_data: Dict) -> Dict:
        """íŠ¹ì • í•­ëª©ì„ ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œë¡œ ë§ˆí¬"""
        for item in data.get("announcements", []):
            if item.get("ìˆœë²ˆ") == ìˆœë²ˆ:
                item["is_detailed"] = True
                item["detail_data"] = detail_data
                item["detail_crawled_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"âœ… ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œ ë§ˆí¬: [{ìˆœë²ˆ}] {item.get('ê³µê³ ëª…', '')[:30]}...")
                break
        
        return data
    
    def get_summary(self, data: Dict) -> Dict:
        """ë°ì´í„° ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        announcements = data.get("announcements", [])
        
        summary = {
            "ì´_í•­ëª©ìˆ˜": len(announcements),
            "ì‹ ê·œ_í•­ëª©ìˆ˜": data.get("new_items_count", 0),
            "ì œê±°ëœ_í•­ëª©ìˆ˜": data.get("removed_old_count", 0),
            "ìƒì„¸í¬ë¡¤ë§_ì™„ë£Œ": len([item for item in announcements if item.get("is_detailed", False)]),
            "ìƒì„¸í¬ë¡¤ë§_ëŒ€ê¸°": len([item for item in announcements if not item.get("is_detailed", False)]),
            "ë§ˆì§€ë§‰_ì—…ë°ì´íŠ¸": data.get("last_updated", ""),
            "ê²€ìƒ‰_í‚¤ì›Œë“œ": data.get("search_keyword", "")
        }
        
        return summary

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª NTIS ë°ì´í„° ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    manager = NTISDataManager()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = [
        {"ê³µê³ ëª…": "í…ŒìŠ¤íŠ¸ ê³µê³  1", "ë¶€ì²˜ëª…": "í…ŒìŠ¤íŠ¸ë¶€ì²˜", "ìƒì„¸_URL": "https://www.ntis.go.kr/rndgate/eg/un/ra/view.do?roRndUid=100&flag=rndList"},
        {"ê³µê³ ëª…": "í…ŒìŠ¤íŠ¸ ê³µê³  2", "ë¶€ì²˜ëª…": "í…ŒìŠ¤íŠ¸ë¶€ì²˜", "ìƒì„¸_URL": "https://www.ntis.go.kr/rndgate/eg/un/ra/view.do?roRndUid=101&flag=rndList"}
    ]
    
    # ë¹„êµ í…ŒìŠ¤íŠ¸
    new_items, existing_items = manager.compare_and_find_new(test_data, "í…ŒìŠ¤íŠ¸")
    
    # ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
    updated_data = manager.update_data_with_new_items(new_items, test_data, "í…ŒìŠ¤íŠ¸")
    
    # ìš”ì•½ ì¶œë ¥
    summary = manager.get_summary(updated_data)
    print(f"\nğŸ“Š ìš”ì•½: {summary}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
