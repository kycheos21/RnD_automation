# NTIS 크롤링 자동화 프로젝트 개발 로드맵 🗺️

## 📅 프로젝트 개요
**목표**: NTIS 웹사이트에서 R&D 정보를 자동으로 수집하여 엑셀 파일로 정리하고 이메일로 발송하는 자동화 시스템 구축

**개발 시작일**: 2025년 9월 2일  
**예상 완료일**: TBD

---

## 🎯 전체 개발 단계

### Phase 1: 환경 설정 및 기초 구성 ✅ **완료**
### Phase 2: 핵심 크롤링 기능 개발 🔄 **진행 중**
### Phase 3: 데이터 처리 및 엑셀 생성
### Phase 4: 자동화 및 스케줄링
### Phase 5: 알림 및 배포

---

## 📋 상세 개발 로드맵

## Phase 1: 환경 설정 및 기초 구성 ✅ **완료**

### 1.1 개발 환경 구축 ✅
- [x] Python 3.13.7 설치
- [x] Node.js v22.18.0 설치  
- [x] Git 2.50.1 설치
- [x] Cursor IDE 설정

### 1.2 프로젝트 초기화 ✅
- [x] 프로젝트 폴더 생성
- [x] Python 가상환경 설정
- [x] Git 저장소 초기화
- [x] GitHub 연동 완료

### 1.3 필수 패키지 설치 ✅
- [x] **pandas** 2.3.2 - 데이터 분석
- [x] **openpyxl** 3.1.5 - 엑셀 파일 처리
- [x] **requests** 2.32.5 - HTTP 요청
- [x] **beautifulsoup4** 4.13.5 - HTML 파싱
- [x] **firecrawl-py** 4.3.1 - 웹 크롤링
- [x] **python-dotenv** 1.1.1 - 환경변수 관리
- [x] **schedule** 1.2.2 - 작업 스케줄링

### 1.4 API 연동 설정 ✅
- [x] Firecrawl 계정 생성
- [x] API 키 발급 및 설정
- [x] `.env` 파일 구성
- [x] API 연결 테스트 성공

### 1.5 문서화 ✅
- [x] `installation-status.md` 작성
- [x] `git-사용설명서.md` 작성
- [x] `development-roadmap.md` 작성 (현재 문서)

**Phase 1 완료율**: 100% ✅

---

## Phase 2: 핵심 크롤링 기능 개발 🔄 **진행 중** (70% 완료)

### 2.1 NTIS 웹사이트 분석 ✅ **완료**
- [x] NTIS 사이트 구조 파악 (R&D 통합공고 페이지)
- [x] 크롤링 대상 페이지 식별 (공고 리스트 + 상세 페이지)
- [x] 데이터 위치 및 형태 분석 (테이블 구조, HWP 첨부파일)
- [x] 접근 제한 및 보안 정책 확인 (공개 데이터)

### 2.2 기본 크롤링 코드 개발 ✅ **완료**
- [x] Selenium 기반 크롤링 시스템 구축 (`test_selenium_ntis.py`)
- [x] 공고 검색 기능 자동화 (본공고, 100개 리스트, 마감일 필터링)
- [x] 페이지 로딩 대기 및 동적 콘텐츠 처리
- [x] 에러 처리 및 재시도 로직 구현

### 2.3 데이터 추출 로직 구현 ✅ **완료**
- [x] 공고 기본 정보 추출 (공고명, 부처명, 마감일, 링크 등)
- [x] 중복 검사 및 신규 공고 필터링
- [x] JSON 데이터베이스 구조 설계 및 구현 (`AnnouncementDatabase`)
- [x] 상세 페이지 HWP 파일 링크 탐지 및 우선순위 선택

### 2.4 HWP 내용 추출 시스템 🔄 **진행 중** (80% 완료)
- [x] "공고문 바로가기" 링크 자동 탐지 및 클릭
- [x] "바로보기" 버튼 자동 탐지 및 클릭  
- [x] 브라우저 창 관리 (새 창 열기/닫기, 원래 창 복귀)
- [x] 텍스트 추출 기본 기능 구현
- [⚠️] **수정 필요**: 실제 HWP 문서 내용 추출 (현재는 공고 페이지 메뉴 추출 중)

### 2.5 크롤링 최적화 ✅ **완료**  
- [x] 페이지 로딩 완료 대기 로직 (키워드 기반 확인)
- [x] 스크린샷 자동 저장 (디버깅용)
- [x] 상세한 로그 시스템 구축
- [x] URL 정리 로직 (`&flag=rndList` 제거)
- [x] **접수일 정렬 로직 개선**: 웹사이트 정렬 버튼 대신 코드 내부에서 접수일 기준 내림차순 정렬 구현
- [x] **데이터 비교 로직 개선**: roRndUid 기준으로 정확한 중복 판별 시스템 구축

**Phase 2 완료율**: 75% (HWP 내용 추출 개선 필요)

---

## Phase 3: AI 요약 및 데이터 처리 🔄 **진행 중** (60% 완료)

### 3.1 AI 요약 시스템 🔄 **진행 중**
- [x] Claude API 연동 기본 구조 구현 (`ClaudeSummarizer`)
- [x] 사업개요 요약 프롬프트 설계
- [x] 구조화된 요약 형식 정의 (사업목적, 지원내용, 지원규모 등)
- [⚠️] **수정 필요**: Claude API 키 문제 해결 (401 인증 오류)

### 3.2 데이터 정제 및 구조화 ✅ **완료**
- [x] JSON 데이터베이스 구조 설계 (기본정보, 사업개요, 메타정보)
- [x] 중복 데이터 제거 (고유 ID 기반)
- [x] 데이터 형식 통일 (날짜, 텍스트 정리)
- [x] 신규/기존 공고 분류 시스템

### 3.3 엑셀 파일 생성 ✅ **완료**
- [x] ExcelReportGenerator 클래스 구현
- [x] 색상 코딩 및 서식 자동 적용
- [x] 하이퍼링크 및 요약 시트 생성
- [x] 독립적인 엑셀 생성 스크립트 (`generate_excel_report.py`)

### 3.4 파일 관리 시스템 ✅ **완료**
- [x] 날짜 기반 파일명 자동 생성
- [x] JSON 데이터베이스 자동 백업
- [x] 출력 폴더 구조 관리 (`output/`, `data/`)
- [x] 스크린샷 자동 저장 (디버깅용)

**Phase 3 완료율**: 60% (AI 요약 시스템 개선 필요)

---

## Phase 4: 자동화 및 스케줄링

### 4.1 스케줄링 시스템 구축
- [ ] 정기 실행 스케줄 설정
- [ ] 크론 작업 또는 Windows 작업 스케줄러 연동
- [ ] 실행 로그 관리
- [ ] 실패 시 재시도 메커니즘

### 4.2 모니터링 및 알림
- [ ] 실행 상태 모니터링
- [ ] 오류 감지 및 알림
- [ ] 성능 지표 수집
- [ ] 대시보드 구성 (선택사항)

### 4.3 설정 관리
- [ ] 설정 파일 구조화
- [ ] 환경별 설정 분리
- [ ] 동적 설정 변경 기능
- [ ] 설정 백업 및 복원

**Phase 4 예상 소요 시간**: 3-5일

---

## Phase 5: 이메일 발송 및 자동화 ⏳ **대기 중**

### 5.1 이메일 발송 시스템 ⏳ **예정**
- [ ] SMTP 서버 설정 (Gmail/Outlook)
- [ ] 이메일 템플릿 작성 (HTML 형식)
- [ ] 엑셀 파일 첨부 기능
- [ ] 발송 실패 처리 및 재시도 로직
- [ ] 수신자 관리 시스템

### 5.2 통합 자동화 시스템 ⏳ **예정**
- [ ] 전체 프로세스 통합 스크립트
- [ ] 일일 자동 실행 스케줄링
- [ ] 에러 발생 시 알림 시스템
- [ ] 실행 로그 및 모니터링

### 5.3 배포 및 운영 ⏳ **예정**
- [ ] 운영 환경 설정 가이드
- [ ] 사용자 매뉴얼 작성
- [ ] 유지보수 가이드 작성
- [ ] 백업 및 복원 절차

**Phase 5 예상 소요 시간**: 3-5일

---

## 📊 전체 진행률

```
Phase 1: ████████████████████ 100% ✅ 완료
Phase 2: ███████████████░░░░░  75% 🔄 진행 중
Phase 3: ████████████░░░░░░░░  60% 🔄 진행 중  
Phase 4: ░░░░░░░░░░░░░░░░░░░░   0% ⏳ 대기 중
Phase 5: ░░░░░░░░░░░░░░░░░░░░   0% ⏳ 대기 중

전체 진행률: 47% (2.35/5 Phase 진행)
```

---

## 🎯 핵심 학습 목표

### 기술적 학습
- [x] **Git 버전 관리** - 브랜치, 커밋, push/pull
- [x] **Python 가상환경** - 패키지 관리
- [x] **API 연동** - REST API 사용법
- [ ] **웹 크롤링** - 동적/정적 콘텐츠 처리
- [ ] **데이터 처리** - pandas를 활용한 데이터 분석
- [ ] **엑셀 자동화** - openpyxl 고급 기능
- [ ] **작업 스케줄링** - 자동화 시스템 구축

### 프로젝트 관리 학습
- [x] **문서 중심 개발** - 체계적인 문서화
- [ ] **단계적 개발** - 점진적 기능 구현
- [ ] **테스트 주도 개발** - 각 단계별 검증
- [ ] **오류 처리** - 견고한 시스템 구축

---

## 📊 **데이터 정렬 및 처리 방식**

### **접수일 기준 정렬 로직**
- **문제**: NTIS 웹사이트의 접수일 정렬 버튼이 불안정하여 Selenium으로 클릭 실패
- **해결**: 웹사이트 정렬 대신 **코드 내부에서 접수일 기준 내림차순 정렬** 구현
- **구현 방식**:
  1. 모든 공고 데이터를 우선 수집
  2. `접수일` 필드를 `datetime` 객체로 파싱 (`YYYY.MM.DD` 형식)
  3. `sorted()` 함수로 접수일 기준 내림차순 정렬 (`reverse=True`)
  4. 최신 공고가 상단에 오도록 배치
- **장점**: 웹사이트 UI 변경에 영향받지 않고 안정적인 정렬 보장

### **데이터 비교 로직 개선** ✅ **2025-10-09 완료**

#### **문제점 및 해결 과정**
기존 데이터와 새로 크롤링한 데이터의 중복 여부를 판단하는 비교 기준을 단계적으로 개선:

| 비교 기준 | 결과 | 문제점 | 정확도 |
|-----------|------|--------|---------|
| **순번** | 신규 30개, 중복 0개 | 웹사이트에서 자동 생성되는 값으로 불안정 | ❌ 부정확 |
| **공고명** | 신규 0개, 중복 30개 | 공고명 변경 시 같은 공고도 신규로 인식 | ⚠️ 보통 |
| **roRndUid** | 신규 0개, 중복 30개 | 각 공고의 고유 식별자로 가장 안정적 | ✅ **최고 정확도** |

#### **최종 구현: roRndUid 기준 비교**
```python
def extract_uid_from_url(self, url: str) -> str:
    """상세_URL에서 roRndUid 추출"""
    match = re.search(r'roRndUid=(\d+)', url)
    return match.group(1) if match else ""

# 비교 로직
existing_uids = set()
for item in existing_announcements:
    uid = self.extract_uid_from_url(item.get("상세_URL", ""))
    if uid:
        existing_uids.add(uid)

# 신규 항목 판별
if item_uid and item_uid not in existing_uids:
    new_items.append(standardized_item)  # 신규 공고
else:
    existing_items.append(standardized_item)  # 기존 공고
```

#### **roRndUid의 장점**
- **고유성**: 각 공고마다 NTIS 데이터베이스의 유일한 식별자
- **안정성**: 공고명이 수정되어도 UID는 변경되지 않음
- **정확성**: 가장 신뢰할 수 있는 중복 판별 기준
- **URL 예시**: `roRndUid=1247708` (상세_URL에서 자동 추출)

### **JSON 저장 구조**
크롤링된 데이터는 다음 구조로 `output/ntis_crawled_raw.json`에 저장:
```json
{
  "순번": int,           // 웹사이트 표시 순번
  "현황": str,           // 공고 상태 (접수중/마감)
  "공고명": str,         // 공고 제목
  "상세_URL": str,       // 상세 페이지 링크 (roRndUid 포함)
  "부처명": str,         // 담당 부처
  "접수일": str,         // 접수 시작일 (YYYY.MM.DD)
  "마감일": str,         // 접수 마감일 (YYYY.MM.DD)
  "크롤링_순번": int     // 접수일 정렬 후 부여한 순번
}
```

## 🚨 **긴급 수정 필요 사항**

### 1. HWP 내용 추출 개선 🔥 **최우선**
- [⚠️] 현재 공고 페이지 메뉴를 추출하고 있음
- [📝] **목표**: "바로보기" 클릭 후 실제 HWP 문서 내용 추출
- [🔧] **방법**: HWP 뷰어 페이지에서 문서 텍스트 영역 탐지

### 2. Claude API 인증 문제 🔥 **긴급**
- [⚠️] 401 invalid x-api-key 오류 발생
- [📝] **목표**: API 키 갱신 또는 재설정
- [🔧] **방법**: 새 API 키 발급 및 환경변수 업데이트

### 3. 페이지 로딩 안정화 ⚡ **중요**
- [⚠️] 상세페이지 로딩 후 바로 추출 시도하는 문제
- [📝] **목표**: 페이지 완전 로딩 후 추출 시작
- [🔧] **방법**: 더 안정적인 대기 로직 구현

---

## 🚀 **신규 개발 예정 사항**

### 1. 요약 내용 JSON 저장 ⏳ **예정**
- [ ] AI 요약 결과를 JSON 데이터베이스에 체계적 저장
- [ ] 사업개요 구조화 (사업목적, 지원내용, 지원규모, 신청대상)
- [ ] 메타정보 추가 (요약 생성일, AI 모델 정보 등)

### 2. 신규 내용 Excel 생성 ⏳ **예정**  
- [ ] 신규 공고만 필터링하여 Excel 생성
- [ ] AI 요약 내용 포함한 상세 보고서
- [ ] 색상 코딩 및 시각화 개선

### 3. 이메일 자동 발송 ⏳ **예정**
- [ ] 일일 신규 공고 Excel 파일 이메일 발송
- [ ] HTML 템플릿 기반 이메일 디자인
- [ ] 발송 실패 시 재시도 및 알림 시스템

---

## 🧪 **단위 테스트 필요 영역**

### HWP 내용 추출 단위 테스트
1. **공고문 바로가기** 링크 탐지 테스트
2. **바로보기** 버튼 클릭 후 페이지 상태 확인
3. **HWP 뷰어 페이지** 텍스트 추출 검증
4. **사업개요 섹션** 식별 정확도 테스트

### API 연동 단위 테스트
1. **Claude API** 연결 및 인증 테스트
2. **요약 품질** 검증 테스트
3. **에러 처리** 시나리오 테스트

---

## 💡 참고사항

### 개발 원칙
1. **단계적 접근** - 한 번에 하나씩 완성
2. **테스트 우선** - 각 기능별로 테스트 코드 작성
3. **문서화 병행** - 코드와 함께 문서 업데이트
4. **에러 처리** - 예외 상황 대비

### 리스크 관리
- **NTIS 사이트 변경** → 정기적 호환성 확인
- **API 제한** → 요청 빈도 조절
- **데이터 품질** → 검증 로직 강화
- **시스템 부하** → 성능 모니터링

---

## 📚 학습 자료

### 추천 문서
- [Firecrawl 공식 문서](https://docs.firecrawl.dev/)
- [pandas 사용자 가이드](https://pandas.pydata.org/docs/)
- [openpyxl 튜토리얼](https://openpyxl.readthedocs.io/)

### 내부 문서
- `installation-status.md` - 환경 설정 상태
- `git-사용설명서.md` - Git 사용법
- `project-overview.md` - 프로젝트 전체 개요

---

---

## 📈 **주요 성과**

### ✅ **완성된 기능들**
1. **Selenium 기반 크롤링 시스템** - 안정적인 동적 페이지 처리
2. **JSON 데이터베이스** - 중복 방지 및 구조화된 데이터 저장
3. **Excel 자동 생성** - 색상 코딩, 하이퍼링크, 요약 시트
4. **브라우저 직접 추출** - "공고문 바로가기" → "바로보기" 자동화
5. **Claude AI 연동** - 구조화된 요약 시스템 (API 키 문제 해결 필요)

### 🎯 **핵심 성취**
- **전체 프로세스 자동화**: 크롤링 → 데이터 저장 → Excel 생성
- **안정적인 페이지 처리**: 동적 콘텐츠 로딩 대기 및 에러 처리
- **모듈화된 구조**: 독립적인 컴포넌트로 유지보수 용이

---

## 🔄 **데이터 관리 구조 개선** ✅ **2025-10-09 완료**

### **기존 문제점**
- 복잡한 `NTISDataManager` 클래스로 인한 코드 복잡성
- 다중 파일 생성 (`raw`, `managed`) 및 관리 부담
- 메서드 호출 오류 및 디버깅 어려움

### **새로운 구조**
```python
# selenium_ntis.py에서 직접 처리
1. 크롤링 → 접수일 정렬
2. 기존 ntis_managed_data.json 로드
3. roRndUid 기반 중복 비교
4. 신규 항목만 new_data.json 저장
5. 전체 데이터 30개 유지 (접수일 내림차순)
```

### **핵심 개선사항**

#### **1. 단순화된 데이터 흐름**
| 이전 | 개선 후 |
|------|---------|
| `selenium_ntis.py` → `NTISDataManager` → 복잡한 메서드 체인 | `selenium_ntis.py` → 직접 JSON 처리 |
| `raw.json` + `managed.json` 이중 관리 | `managed_data.json` 단일 관리 |
| 클래스 의존성 및 메서드 오류 | 직접적인 파일 처리 |

#### **2. roRndUid 기반 정확한 비교**
```python
# 기존: 순번 → 공고명 → roRndUid (단계적 개선)
def extract_uid_from_url(url):
    match = re.search(r'roRndUid=(\d+)', url)
    return match.group(1) if match else ""

# 비교 로직
existing_uids = set()
for item in existing_data:
    uid = extract_uid_from_url(item.get("상세_URL", ""))
    if uid:
        existing_uids.add(uid)
```

#### **3. 유연한 데이터 관리**
- **선택적 복원**: `managed_data.json`에서 항목 삭제 시 다음 실행에서 해당 항목만 `new_data.json`에 저장
- **30개 항목 유지**: 기존 + 신규 합쳐서 접수일 기준 최신 30개 자동 선별
- **접수일 정렬**: `datetime` 객체 변환으로 정확한 날짜 정렬

#### **4. 실행 결과 검증**
```
테스트 시나리오: managed_data.json에서 3개 항목 삭제
결과:
📂 기존 데이터: 27개
🆕 신규 항목: 3개 (삭제했던 항목들이 정확히 인식됨)
🔄 중복 항목: 27개
💾 new_data.json: 삭제했던 3개 항목만 저장
```

### **장점**
- ✅ **코드 단순성**: 클래스 의존성 제거, 직관적인 흐름
- ✅ **정확성**: roRndUid 기반 100% 정확한 중복 판별
- ✅ **유연성**: 원하는 항목 삭제 후 선택적 복원 가능
- ✅ **안정성**: 메서드 호출 오류 없는 직접 처리
- ✅ **효율성**: 신규 항목 없으면 불필요한 파일 생성 안 함

---

*이 로드맵은 프로젝트 진행에 따라 지속적으로 업데이트됩니다.*  
*마지막 업데이트: 2025년 10월 9일*
